{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the importance of features from dataset from 4 differnt techniques - LOFO - XGB - LGBM - RandomForest\n",
    "- Uses Cross-Validation for Gridsearch of varying parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine as ce\n",
    "from datetime import *\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "from sklearn import *\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from itertools import *\n",
    "import re\n",
    "import shap\n",
    "import random\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from lofo import LOFOImportance, Dataset, plot_importance\n",
    "import time\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features that are needed to be checked for importance \n",
    "features = [\"feature_1\",\n",
    "               \"feature_2\", \n",
    "               \"feature_3\", \n",
    "               \"feature_4\", \n",
    "               \"feature_5\", \n",
    "               \"feature_6\", \n",
    "               \"feature_7\"]\n",
    "\n",
    "# list of all target metrics that migth be used for further analysis;  small letters\n",
    "targets = ['main_target_metric', \n",
    "           'target_metric_1', \n",
    "           'target_metric_2']            \n",
    "\n",
    "target = 'main_target_metric'                   # target metric that will be used by decision tree;  small letters\n",
    "\n",
    "clip_metric = 'yes'                             # Clip metric values : 'yes' or 'no'\n",
    "clip_metric_limit = [0, 3000]                   # Min, Max values to limit metric\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"'2023-05-01'\"                         # Date is inclusive\n",
    "end_date = \"'2023-08-15'\"                           # Date is inclusive\n",
    "\n",
    "ip = 'xxx.xx.xxx.xxx'                               # IP address\n",
    "port = xxxx\n",
    "user = 'username'                                   # username\n",
    "pass_ai = 'password'                                # password user \n",
    "db = 'dbname'                                       # name of schema\n",
    "\n",
    "main_table = 'table_name'                           # name of table\n",
    "filter = \"isrelevant=1\"                             # filter for table\n",
    "time_column = 'calltime'                            # column name in table for calltime\n",
    "prefix = 'week'                                     # For week-wise ('week') or daily ('date') stability charts for created bins      \n",
    "\n",
    "dates_to_filter = \"('2022-05-01')\"                  # If some dates needs to be removed from data\n",
    "\n",
    "# There could be many different ways NAs could be present in dataset. All the following will be converted to numpy.nan\n",
    "nas_to_replace = ['NA', 'NULL', 'NUL', 'NaN', '[NA]', 'nan', 'NAN', ' ']\n",
    "\n",
    "# unexpected_numeric_values = {'original' : 999999999.0, 'replace' : 999999.0}\n",
    "# unexpected_string_values = {'original' : '999999999.0', 'replace' : '999999.0'}\n",
    "# special_char = ['$', '&', '%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = 10                                   # Number of top features for selection\n",
    "feature_importance_technique = 'LOFO'               # Technique for calculating feature importance; ['LOFO', 'RandomForest', 'XGB', 'LGBM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is fetched through following sql query\n",
    "\n",
    "query = \"Select \" + time_column + ','+','.join(features)+','+','.join(targets)+\" from \" + main_table + \" where \" + time_column + \" >= \" + start_date +\" and \" + time_column + \" <= \" + end_date +\" and \" + filter +\" and and \" +time_column+\" not in \"+dates_to_filter+\" ;\"\n",
    "print(query)\n",
    "\n",
    "ai_conn = ce('mysql://'+user+':'+pass_ai+'@'+ip+':'+str(port)+'/'+db)\n",
    "data = pd.read_sql(query,ai_conn)\n",
    "\n",
    "print(\"data fetched successfully : \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names to lower case\n",
    "data = data.rename(columns = lambda x: x.lower())\n",
    "\n",
    "# replace desired values with NAs\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(nas_to_replace, np.nan)\n",
    "    \n",
    "# Clip metric if required\n",
    "if clip_metric == 'yes':\n",
    "    data[target] = data[target].astype(float).clip(clip_metric_limit[0], clip_metric_limit[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell 4 different functions for 4 techniques are defined - LOFO - XGB - LGBM - RandomForest\n",
    "# params defined here are arbitrary and should be changed according to the size of data and granularity desired\n",
    "# SHAP technique is used to determine feature importance from grid searches of XGB and lgbm\n",
    "\n",
    "# Cross validation is set for KFold with 4 splits. Change according to need\n",
    "cv = KFold(n_splits = 4, shuffle=False)\n",
    "\n",
    "# SHAP is a widely used tool to explain machine learning models. Here its used for XGB and lgbm\n",
    "def shap_importance(model, X, y):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    return shap_values\n",
    "\n",
    "# LOFO refers to Leave-One-Feature-Out technique of importance. \n",
    "def lofo_importance(data ,X, target):\n",
    "    params = {'n_estimators':[100, 200, 300], 'learning_rate' : [0.2, 0.1, 0.01], 'num_leaves' : [50, 100, 150], 'n_jobs' : [40, 50, 60]}    \n",
    "    estimator = lgb.LGBMRegressor()\n",
    "    grid_search = GridSearchCV(estimator, params, cv = cv)\n",
    "    grid_search.fit(X, data[target])\n",
    "    dataset = Dataset(df = data, target = target, features = X.columns)\n",
    "    lofo_imp = LOFOImportance(dataset, cv = cv, scoring = 'r2', model = grid_search.best_estimator_)\n",
    "    importance = lofo_imp.get_importance()\n",
    "#     plot_importance(importance, figsize=(12,30))\n",
    "\n",
    "    return importance\n",
    "\n",
    "# XGB is widely used gradient boosting technique. It requires all its feature values to be float. \n",
    "def xgb_importance(X, y):\n",
    "    params = {'n_estimators':[100, 200, 300], 'learning_rate':[0.2, 0.1, 0.01], 'max_depth' : [2,5,7]}\n",
    "    model = XGBRegressor()\n",
    "    grid_search = GridSearchCV(model, params, cv = cv)\n",
    "    grid_search.fit(X, y)\n",
    "    importance = shap_importance(grid_search.best_estimator_, X, y)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# LGBM is Light Gradient Boosting Method. \n",
    "def lgbm_importance(X, y):\n",
    "    params = {'n_estimators':[100, 200, 300], 'learning_rate':[0.2, 0.1, 0.01], 'max_depth' : [2,5,7], 'num_leaves' : [50, 100, 150]}\n",
    "    model = LGBMRegressor(importance_type = 'gain')\n",
    "    grid_search = GridSearchCV(model, params, cv = cv)\n",
    "    grid_search.fit(X, y)\n",
    "    importance = shap_importance(grid_search.best_estimator_, X, y)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# Random Forest is one of the widely used Machine Learning technique. \n",
    "def randomforest_importance(X, y):\n",
    "    params = {'n_estimators':[100, 200, 300], 'max_depth' : [2,5,7], 'min_samples_split' : range(500,5000,500)}\n",
    "    rf = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(rf, params, cv = cv)\n",
    "    grid_search.fit(X, y)\n",
    "    importances = grid_search.best_estimator_.feature_importances_\n",
    "    feature_importances = pd.DataFrame({'Feature' : X.columns, 'Importance' : importances})\n",
    "    feature_importances = feature_importances.sort_values('Importance', ascending = False)\n",
    "    \n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of dataset \n",
    "final_data = data.copy()\n",
    "\n",
    "# XGB requires all its feature values to be float datatype. The categorical features are label encoded if XGB is used. Provide a list of categoric features in cate_cols list below to be lable encoded.\n",
    "cate_cols = [] # to be provided by user\n",
    "# Be cautious that label encoding creates large dataset which might hinder performance. Avoid if large dataset is used or if there are many feature values\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for x in cate_cols:\n",
    "    if x != target:\n",
    "        final_data[x] = [str(v) for v in final_data[x]]\n",
    "        final_data[x] = le.fit(final_data[x]).transform(final_data[x])\n",
    "    \n",
    "    if feature_importance_technique == 'XGB' :\n",
    "        final_data[x] = final_data[x].astype('float')\n",
    "    else :\n",
    "        final_data[x] = final_data[x].astype('category')\n",
    "final_data = final_data.fillna(0)\n",
    "\n",
    "y = final_data[target]\n",
    "drop = [target, time_column]\n",
    "X = final_data.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()            # Time check to check performance\n",
    "\n",
    "if feature_importance_technique == 'LOFO':\n",
    "    feature_importance = lofo_importance(final_data, X, target)\n",
    "\n",
    "    important_features = feature_importance['feature'].head(top_features).to_list()\n",
    "\n",
    "# Uncomment following lines if complete list of feature importance is required\n",
    "#     important_features_data = final_data[important_features] \n",
    "#     important_features_data.reset_index(inplace=True)\n",
    "#     important_features_data = important_features_data.drop('index', axis=1)\n",
    "\n",
    "elif feature_importance_technique == 'XGB':\n",
    "    values = xgb_importance(X, y)\n",
    "    importance = abs(values).mean(axis=0)\n",
    "    feature_importance = sorted(zip(X.columns, importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    important_features = [feat for feat, _ in feature_importance[:top_features]]\n",
    "\n",
    "# Uncomment following lines if complete list of feature importance is required\n",
    "#     important_features_data = final_data[important_features] \n",
    "\n",
    "\n",
    "elif feature_importance_technique == 'LGBM':\n",
    "    values = lgbm_importance(X, y)\n",
    "    importance = abs(values).mean(axis=0)\n",
    "    feature_importance = sorted(zip(X.columns, importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    important_features = [feat for feat, _ in feature_importance[:top_features]]\n",
    "\n",
    "# Uncomment following lines if complete list of feature importance is required\n",
    "#     important_features_data = final_data[important_features] \n",
    "\n",
    "else : \n",
    "    feature_importance = randomforest_importance(X, y)\n",
    "\n",
    "    important_features = feature_importance['Feature'].head(top_features).to_list()\n",
    "\n",
    "# Uncomment following lines if complete list of feature importance is required\n",
    "#     important_features_data = final_data[important_features] \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time is \", round(end_time - start_time, 4), 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the plot of top features with thier importance\n",
    "\n",
    "if feature_importance_technique == 'LOFO' :\n",
    "    plot_importance(feature_importance, figsize=(10,10))\n",
    "\n",
    "elif feature_importance_technique == 'XGB' :\n",
    "    sns.set_style('whitegrid')\n",
    "    shap.summary_plot(values, features = X, feature_names = X.columns, plot_type = 'bar', plot_size = 0.5)\n",
    "\n",
    "elif feature_importance_technique == 'LGBM' :\n",
    "    sns.set_style('whitegrid')\n",
    "    shap.summary_plot(values, features = X, feature_names = X.columns, plot_type = 'bar', plot_size = 0.5)\n",
    "\n",
    "else :\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set(rc={'figure.figsize':(11,10)})\n",
    "    sns.barplot(x = 'Importance', y = 'Feature', data = feature_importance, color='b')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important features\n",
    "\n",
    "print(\"List of top \",top_features,\" features from \",feature_importance_technique,\" technique\")\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
