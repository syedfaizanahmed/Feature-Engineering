{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create feature summary and python feature profiling report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine as ce\n",
    "from datetime import *\n",
    "from sklearn import *\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features that needs to be profiled\n",
    "\n",
    "features = [\"feature_1\",\n",
    "               \"feature_2\", \n",
    "               \"feature_3\", \n",
    "               \"feature_4\", \n",
    "               \"feature_5\", \n",
    "               \"feature_6\", \n",
    "               \"feature_7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"'2023-05-01'\"                         # Date is inclusive\n",
    "end_date = \"'2023-08-15'\"                           # Date is inclusive\n",
    "\n",
    "ip = 'xxx.xx.xxx.xxx'                               # IP address\n",
    "port = xxxx\n",
    "user = 'username'                                   # username\n",
    "pass_ai = 'password'                                # password user \n",
    "db = 'dbname'                                       # name of schema\n",
    "\n",
    "main_table = 'table_name'                           # name of table\n",
    "filter = \"isrelevant=1 and on_off = 0\"              # filter for table\n",
    "time_column = 'calltime'                            # column name in table for calltime    \n",
    "\n",
    "dates_to_filter = \"('2022-05-01')\"                  # If some dates needs to be removed from data\n",
    "\n",
    "\n",
    "\n",
    "# unexpected_numeric_values = {'original' : 999999999.0, 'replace' : 999999.0}\n",
    "# unexpected_string_values = {'original' : '999999999.0', 'replace' : '999999.0'}\n",
    "\n",
    "\n",
    "# There could be many different ways NAs could be present in dataset. All the following will be converted to numpy.nan\n",
    "nas_to_replace = ['NA', 'NULL', 'NUL', 'NaN', '[NA]', 'nan', 'NAN', ' ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit defined to create a minimized version of feature profile report\n",
    "features_limit = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is fetched through following sql query\n",
    "\n",
    "query = \"Select \" + time_column + ','+','.join(features)+\" from \" + main_table + \" where \" + time_column + \" >= \" + start_date +\" and \" + time_column + \" <= \" + end_date +\" and \" + filter +\" and and \" +time_column+\" not in \"+dates_to_filter+\" ;\"\n",
    "print(query)\n",
    "\n",
    "ai_conn = ce('mysql://'+user+':'+pass_ai+'@'+ip+':'+str(port)+'/'+db)\n",
    "data = pd.read_sql(query,ai_conn)\n",
    "\n",
    "print(\"data fetched successfully : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names to lower case\n",
    "data = data.rename(columns = lambda x: x.lower())\n",
    "\n",
    "# replace desired values with NAs\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(nas_to_replace, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Summary and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_values_N = 5                              # Get the top N values for each feature\n",
    "null_perc_benchmark = 70                      # Features with NULL percentage above this benchmark will be removed from binning\n",
    "top_1_value_benchmark = 60                    # Features with top 1 value percentage above this benchmark will be removed from binning\n",
    "\n",
    "special_char = ['$', '&', '%']                 # Any special charcters that needs to be checked for as a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, following stats summary is calculated for each feature\n",
    "# - unique values - top N values - top N values perc - top 1 value perc - perc of \"NA\" - perc of nulls - perc of special char\n",
    "\n",
    "unique_values = []\n",
    "topN_values = []\n",
    "topN_values_pct = []\n",
    "perc_topN_values = []\n",
    "perc_top1_value = []\n",
    "\n",
    "perc_stringNA =[]\n",
    "perc_null=[]\n",
    "perc_special=[]\n",
    "\n",
    "data1 = data.copy()\n",
    "perc_null = data1[data1.columns].isnull().sum()*100/len(data1)\n",
    "perc_null_df = perc_null.to_frame().reset_index()\n",
    "\n",
    "dt_types = data1[data1.columns].dtypes\n",
    "for col in data1.columns:\n",
    "    series = data1[col].value_counts(normalize=True).to_frame().reset_index()   #fetching all the unique values for each column in descending order\n",
    "    unique_values.append(len(series[col]))                                      #number of unique values\n",
    "    topN_values.append(list(series[0:top_values_N]['index']))                   #listing down top N values\n",
    "    topN_values_pct.append(list(100*round(series[0:top_values_N][col],2)))      #listing down top N values pct\n",
    "    perc_topN_values.append(100*round(series[0:top_values_N][col].sum(),2))     #Sum of Pct of top N Values\n",
    "    perc_top1_value.append(100*round(series[0:1][col].sum(),2))                 #pct of top value\n",
    "\n",
    "    if dt_types[col]=='<M8[ns]':                                                #setting NA/special_char as zero if data type is datetime\n",
    "        perc_stringNA.append(0)\n",
    "        perc_special.append(0)\n",
    "    else:\n",
    "        perc_stringNA.append(0 if len(series[series['index'].isin(['NA'])][col])==0 \n",
    "                     else round(100*series[series['index'].isin(['NA'])][col].sum(),2))             #setting as 0 if number of NA are zero else pct of NAs\n",
    "    \n",
    "        perc_special.append(0 if len(series[series['index'].isin(special_char)][col])==0 \n",
    "                        else round(100*series[series['index'].isin(special_char)][col].sum(),2))    #setting as 0 if number of special are zero else pct of special\n",
    "\n",
    "data_summary = pd.DataFrame({'columns': data1.columns,\n",
    "                             'unique_values':unique_values,\n",
    "                             'topN_values':topN_values,\n",
    "                             'perc_topN_values': perc_topN_values,\n",
    "                             'perc_top1_value': perc_top1_value,\n",
    "                             'perc_stringNA': perc_stringNA,\n",
    "                             'perc_null': round(perc_null_df[0],2),\n",
    "                             'perc_special': perc_special})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data summary table as excel file on working directory\n",
    "data_summary.to_excel(\"./data_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python feature profiling report is generated with all sections if number of features is less than the feature_limit\n",
    "# Else a minimal report is generated\n",
    "# It is advised to use minimal report if number of features exceed 10-15 bcz of heavy and cluttered interactions and correlations section of report at higher number of features\n",
    "\n",
    "data = data[features]\n",
    "\n",
    "if len(data.columns) > features_limit:\n",
    "#     profile = ProfileReport(data, title = 'Minimal Features Profiling', interactions=None, correlations=None)\n",
    "    profile = ProfileReport(data, title = 'Minimal Features Profiling', minimal=True)\n",
    "    profile.to_file('Profiling_Features_minimal_report.html')\n",
    "else:\n",
    "    profile = ProfileReport(data, title = 'Features Profiling')\n",
    "    profile.to_file('Profiling_Features_report.html')\n",
    "\n",
    "# Report is generated and saved in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
