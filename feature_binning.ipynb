{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Classify featues into Numeric, Categoric and Datetime feature\n",
    "##### 2. Create Bins for based on DecisionTrees\n",
    "##### 3. Generate stability plots for created Bins\n",
    "##### 4. Generate statements to be plugged-in the table view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses Decision Trees to create bins for Numerical and Cateorical Features\n",
    "- Recommended to use OFF data since it doesnt contain affected metric\n",
    "- Numbers of Bins created can be controlled/limited by data volume and hyperparameters\n",
    "- Metric to create Bins and Stability Plots of created Bins can be based on same or different metrics\n",
    "- If Binomial Metric is used for binning; change 'DecisionTreeRegressor' to 'DecisionTreeClassifier' and scoring from 'r2' to 'accuracy'\n",
    "- Naming convention in case statements are generic; change to reflect actual naming convention for your need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as pp\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import sqlalchemy as sa\n",
    "from datetime import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sqlalchemy import create_engine as ce\n",
    "import json\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unbinned features that are needed to be binned \n",
    "features = [\"unbinned_feature_1\",\n",
    "               \"unbinned_feature_2\", \n",
    "               \"unbinned_feature_3\", \n",
    "               \"unbinned_feature_4\", \n",
    "               \"unbinned_feature_5\", \n",
    "               \"unbinned_feature_6\", \n",
    "               \"unbinned_feature_7\"]\n",
    "\n",
    "# list of all target metrics that migth be used for further analysis;  small letters\n",
    "targets = ['main_target_metric', \n",
    "           'target_metric_1', \n",
    "           'target_metric_2']            \n",
    "\n",
    "target = 'main_target_metric'                   # target metric that will be used by decision tree;  small letters\n",
    "\n",
    "clip_metric = 'yes'                             # Clip metric values : 'yes' or 'no'\n",
    "clip_metric_limit = [0, 3000]                   # Min, Max values to limit metric\n",
    "\n",
    "\n",
    "# Identify datetime col, if prefix/postfix already known (and does not appear in any other type of feature name)\n",
    "# Any datetime feature name prefix/postfix except time_column\n",
    "datetime_cols_id = ['dt_']                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended not to use large datasets/large time periods. Often 30-35 days of data is enough to create reasonable bins\n",
    "# Very large datasets would end up creating granular bins; If using large datasets, then pruning should be done accordingly\n",
    "\n",
    "start_date = \"'2023-05-01'\"                         # Date is inclusive\n",
    "end_date = \"'2023-08-15'\"                           # Date is inclusive\n",
    "\n",
    "ip = 'xxx.xx.xxx.xxx'                               # IP address\n",
    "port = xxxx\n",
    "user = 'username'                                   # username\n",
    "pass_ai = 'password'                                # password user \n",
    "db = 'dbname'                                       # name of schema\n",
    "\n",
    "main_table = 'table_name'                           # name of table\n",
    "filter = \"isrelevant=1 and on_off = 0\"              # filter for table\n",
    "time_column = 'calltime'                            # column name in table for calltime\n",
    "prefix = 'week'                                     # For week-wise ('week') or daily ('date') stability charts for created bins      \n",
    "\n",
    "dates_to_filter = \"('2022-05-01')\"                  # If some dates needs to be removed from data\n",
    "\n",
    "# There could be many different ways NAs could be present in dataset. All the following will be converted to numpy.nan\n",
    "nas_to_replace = ['NA', 'NULL', 'NUL', 'NaN', '[NA]', 'nan', 'NAN', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new case statements are created for binned features that are provided in a text file of the following name and identity\n",
    "\n",
    "dataset_name = 'my_data'                      # Name of table/dataset, to be used as postfix for binned features for identification\n",
    "statement_filename = 'my_data'                # Name to be added as postfix in case statement file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is fetched through following sql query\n",
    "\n",
    "query = \"Select \" + time_column + ','+','.join(features)+','+','.join(targets)+\" from \" + main_table + \" where \" + time_column + \" >= \" + start_date +\" and \" + time_column + \" <= \" + end_date +\" and \" + filter +\" and and \" +time_column+\" not in \"+dates_to_filter+\" ;\"\n",
    "print(query)\n",
    "\n",
    "ai_conn = ce('mysql://'+user+':'+pass_ai+'@'+ip+':'+str(port)+'/'+db)\n",
    "data = pd.read_sql(query,ai_conn)\n",
    "\n",
    "print(\"data fetched successfully : \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names to lower case\n",
    "data = data.rename(columns = lambda x: x.lower())\n",
    "\n",
    "# replace desired values with NAs\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].replace(nas_to_replace, np.nan)\n",
    "    \n",
    "# Clip metric if required\n",
    "if clip_metric == 'yes':\n",
    "    data[target] = data[target].astype(float).clip(clip_metric_limit[0], clip_metric_limit[1])\n",
    "    \n",
    "# drop [targets] and [time_column] from data to classify features in the next cell\n",
    "data1 = data.drop(targets, axis=1)\n",
    "data1 = data1.drop(time_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from dataset are identified by trying data types and checking the ValueErrors\n",
    "\n",
    "numeric_cols = []                       # Columns only containing numeric datatype\n",
    "categorical_cols = []                   # Columns only containing categorical datatype\n",
    "datetime_cols = []                      # Columns only containing datetime datatype\n",
    "mixed_cols = []                         # Columns containing multiple dataypes. Will be binned as categorical columns later in the script\n",
    "\n",
    "for col in data1.columns:\n",
    "    column_values = data1[col].dropna()\n",
    "    \n",
    "# If there are any errors raised when changing the values to numeric then dont add as numeric feature\n",
    "    try:\n",
    "        values_numeric = pd.to_numeric(column_values, errors='raise')\n",
    "        numeric_cols.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        mixed_cols.append(col)\n",
    "        \n",
    "# Check types of each column value and classify if their dtypes are number or datetime, else its categorical\n",
    "    unique_types = set(type(value).__name__ for value in column_values)\n",
    "\n",
    "# If value type is just 1 then its uniquely one type else it would be classified as mixed\n",
    "    if len(unique_types) ==1:\n",
    "        if np.issubdtype(column_values.dtype, np.number):\n",
    "            numeric_cols.append(col)\n",
    "        elif np.issubdtype(column_values.dtype, np.datetime64):\n",
    "            datetime_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    else:\n",
    "        mixed_cols.append(col)\n",
    "        \n",
    "for col in mixed_cols:\n",
    "    column_values = data1[col].dropna()\n",
    "    try:\n",
    "        values_datetime = pd.to_datetime(column_values, errors='raise')\n",
    "        datetime_cols.append(col)\n",
    "    except (ValueError, TypeError):\n",
    "        categorical_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any feature repeated in multiple feature types\n",
    "repeated_features = []\n",
    "\n",
    "for list1 in [numeric_cols, categorical_cols, mixed_cols, datetime_cols]:\n",
    "    repeated_features += [x for x in set(list1) if list1.count(x) > 1]\n",
    "\n",
    "if len(datetime_cols_id) >=1:\n",
    "    for col in data1.columns:\n",
    "        for i in datetime_cols_id:\n",
    "            if i in col and col not in datetime_cols:\n",
    "                datetime_cols.append(col)\n",
    "\n",
    "numeric_cols = [x for x in numeric_cols if x not in datetime_cols]\n",
    "categorical_cols = [x for x in categorical_cols if x not in datetime_cols]\n",
    "mixed_cols = [x for x in mixed_cols if x not in datetime_cols]\n",
    "\n",
    "for col in mixed_cols:\n",
    "    if col in numeric_cols:\n",
    "        numeric_cols.remove(col)\n",
    "    if col in categorical_cols:\n",
    "        categorical_cols.remove(col)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in categorical_cols:\n",
    "        categorical_cols.remove(col)\n",
    "\n",
    "repeated_features = list(set(repeated_features))\n",
    "numeric_cols = list(set(numeric_cols))\n",
    "categorical_cols = list(set(categorical_cols))\n",
    "mixed_cols = list(set(mixed_cols))\n",
    "datetime_cols = list(set(datetime_cols))\n",
    "\n",
    "print('Repeated_features b/w categorical and mixed features : ',repeated_features , \n",
    "      '\\nNumeric Features count : ',len(numeric_cols), \n",
    "      '\\nCategorical Features count : ',len(categorical_cols),\n",
    "      '\\nMixed/Categorical Features count : ',len(mixed_cols), \n",
    "      '\\nDatetime Features count : ', len(datetime_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert dataframe into 1D array list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create bins for both Numerical and Categorical features\n",
    "# data_type refers to the type of feature : \"numerical\" or \"categorical\"\n",
    "# daa refers to tge data table\n",
    "# parameters refer to the hyperparameters used for gridsearch\n",
    "# cols refer to the type of features : \"numerical_cols\" or \"categorical_cols\"\n",
    "\n",
    "def feature_bins(data_type, data, params, cols):\n",
    "\n",
    "# describe Tree Model\n",
    "    tree_model= DecisionTreeRegressor()\n",
    "    cv = GridSearchCV(tree_model, param_grid=params, scoring='r2', cv=5)\n",
    "\n",
    "    binning_data = data[['Num', time_column]]\n",
    "\n",
    "# go through each feature iteratively\n",
    "    for i in tqdm(cols):\n",
    "        train = data[['Num',i, target]]\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "# convert Categorical values into coded values\n",
    "        if data_type == 'categorical':\n",
    "            train[i+'_coded'] = train[i].astype('category').cat.codes\n",
    "            cv.fit(train[i+'_coded'].values.reshape(-1,1), train[target].values.reshape(-1,1))\n",
    "        else:\n",
    "            cv.fit(train[i].values.reshape(-1,1), train[target].values.reshape(-1,1))\n",
    "\n",
    "# check for best hyperparametric values for Tree\n",
    "        tree_model = DecisionTreeRegressor(max_depth= cv.best_params_['max_depth'], min_samples_leaf= cv.best_params_['min_samples_leaf'], min_impurity_decrease=cv.best_params_['min_impurity_decrease'])\n",
    "\n",
    "# fit Tree to create bins\n",
    "        if data_type == 'categorical':\n",
    "            tree_model.fit(train[i+'_coded'].values.reshape(-1,1), train[target].values.reshape(-1,1))\n",
    "\n",
    "            bins = sorted(set(tree_model.tree_.threshold))\n",
    "            labels = ['bin'+str(j) for j in range(len(bins)-1)]\n",
    "            train[str(i)+'_Tree_Leaf'] = pd.cut(train[i+'_coded'], bins=bins, labels=labels)\n",
    "\n",
    "            binning_data = pd.merge(binning_data, train[['Num', i, i+'_coded', i+'_Tree_Leaf']], on='Num', how='left')\n",
    "\n",
    "        else:\n",
    "            tree_model.fit(train[i].values.reshape(-1,1), train[target].values.reshape(-1,1))\n",
    "\n",
    "            on_leaf = tree_model.apply(train[i].values.reshape(-1,1))\n",
    "            train[i+'_Bins'] = on_leaf\n",
    "\n",
    "            binning_data = pd.merge(binning_data,train[['Num',i, i+'_Bins']], on = 'Num', how='left')\n",
    "\n",
    "# combine feature bin values\n",
    "    binning_data = pd.merge(binning_data,data[['Num',target]], on = 'Num', how='inner')\n",
    "    \n",
    "    return binning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot stability plots of the binned features\n",
    "# data_type refers to the type of feature : \"numerical\" or \"categorical\"\n",
    "# table refers to the generated table that contains binned feature values\n",
    "# data refers to hte data table\n",
    "# names refer to the name of features binned\n",
    "# prefix refers to the time duration used for stability plots : \"week\" or :\"date\"\n",
    "# fig_list refer to the variable in which stability plots are saved\n",
    "\n",
    "def plotting(final_data, feature_list, prefix, figs): \n",
    "# Creating stability plots. \n",
    "# There are 3 plots in a single column. 1- Proportion of individual value 2- Granularity wise average target value 3- Granulairty wise rank of each value in feature\n",
    "\n",
    "    # going through list of features one by one\n",
    "    for j in tqdm(range(len(feature_list))):\n",
    "\n",
    "        # create a subplot with 1 row and 3 columns\n",
    "        fig = make_subplots(rows=1, cols=3, subplot_titles = [prefix + \"proportion\", prefix + \"average target\", prefix + \"rank\"])    \n",
    "        feature = feature_list[j]\n",
    "        \n",
    "        # Calculating proportion of each distinct value in feature\n",
    "        plotting_data = final_data.groupby([prefix, feature]).size().unstack()\n",
    "        week_vols = []\n",
    "        for i in range(len(plotting_data)):\n",
    "            week_vols.append(plotting_data.iloc[i].sum())\n",
    "            plotting_data.iloc[i] = plotting_data.iloc[i]/week_vols[i] * 100\n",
    "\n",
    "        # Filling NaNs to 0 in plotting data\n",
    "        plotting_data = plotting_data.fillna(0)\n",
    "\n",
    "        # Creating plots and saving in fig variable\n",
    "        for i, col in enumerate(plotting_data.columns):\n",
    "                fig.add_trace(go.Bar(name = col, x=plotting_data.index, y=plotting_data[col]), row=1, col=1)\n",
    "\n",
    "        # Calculating average target metric for each distinct feature value\n",
    "        plotting_data = final_data.groupby([prefix, feature])[target].mean().unstack()\n",
    "        plotting_data = plotting_data.fillna(0)\n",
    "        for i, col in enumerate(plotting_data.columns):\n",
    "            fig.add_trace(go.Scatter(x = plotting_data.index, y=plotting_data[col], mode='lines+markers', name=col), row=1, col=2)\n",
    "\n",
    "        # Calculating rank (based on average target metric value) for each distinct feature value\n",
    "        plotting_data = final_data.groupby([prefix, feature])[target].mean().unstack()\n",
    "        plotting_data = plotting_data.fillna(0)\n",
    "        for i in range(len(plotting_data)):\n",
    "            plotting_data.iloc[i] = ss.rankdata(plotting_data.iloc[i])\n",
    "\n",
    "        for i, col in enumerate(plotting_data.columns):\n",
    "            fig.add_trace(go.Scatter(x = plotting_data.index, y=plotting_data[col], mode='lines+markers', name=col), row=1, col=3)\n",
    "\n",
    "\n",
    "        fig.update_xaxes(title_text=prefix)\n",
    "        fig.update_yaxes(title_text=\"Proportion\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Average \"+target, row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Rank\", row=3, col=1)\n",
    "        fig.update_layout(title=feature)\n",
    "        fig.update_layout(barmode='stack')\n",
    "        \n",
    "        figs.append(fig)\n",
    "        return figs\n",
    "\n",
    "\n",
    "# to convert figs plots into hml file\n",
    "\n",
    "def figures_to_html(figs, filename = 'dashboard.html'):\n",
    "    dashboard = open(filename, 'w')\n",
    "    dashboard.write(\"<html><head></head><body>\" + \"\\n\")\n",
    "    for fig in figs:\n",
    "        inner_html = fig.to_html().split('<body>')[1].split('</body>')[0]\n",
    "        dashboard.write(inner_html)\n",
    "    dashboard.write(\"</body>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove time_column from datetime_cols if exists\n",
    "if time_column in datetime_cols:\n",
    "    datetime_cols.remove(time_column)\n",
    "\n",
    "# make new data table with just numerical cols, datetime cols, calltime and target metrics\n",
    "numeric_data = data[numeric_cols+targets+datetime_cols+[time_column]]\n",
    "numeric_data[target] = numeric_data[target].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime cols into numerical cols with \"days\", 'weeks' and 'months' duration difference\n",
    "if len(datetime_cols) >=1:\n",
    "    current_date = pd.to_datetime(pd.Timestamp.today().date())\n",
    "    processed_datetime_cols = []\n",
    "\n",
    "    for col in datetime_cols:\n",
    "        numeric_data[col] = pd.to_datetime(numeric_data[col], errors = 'coerce')\n",
    "        numeric_data['days_since_'+str(col)] = (current_date - numeric_data[col]).dt.days\n",
    "        numeric_data['weeks_since_'+str(col)] = (current_date - numeric_data[col]).dt.days //7\n",
    "        numeric_data['months_since_'+str(col)] = (current_date.year - numeric_data[col].dt.year)*12 + (current_date.month - numeric_data[col].dt.month)\n",
    "        processed_datetime_cols.append(['days_since_'+str(col), 'weeks_since_'+str(col), 'months_since_'+str(col)])\n",
    "\n",
    "    numeric_data = numeric_data.drop(datetime_cols, axis=1)\n",
    "\n",
    "    processed_datetime_cols = flatten(processed_datetime_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target metrics and time_column from numerical cols\n",
    "numeric_cols = numeric_data.columns.to_list()\n",
    "for i in targets:\n",
    "    if i in numeric_cols:\n",
    "        numeric_cols.remove(i)\n",
    "        \n",
    "numeric_cols = [x for x in numeric_cols if x not in targets]\n",
    "numeric_cols.remove(time_column)\n",
    "\n",
    "# processing of numerical data before binning\n",
    "for f in numeric_cols:\n",
    "    numeric_data[f] = pd.to_numeric(numeric_data[f], errors='coerce')\n",
    "    numeric_data[f] = numeric_data[f].astype(float).apply(lambda x: round(x, 1))\n",
    "\n",
    "numeric_data['Num'] = numeric_data.index\n",
    "\n",
    "print('After processing')\n",
    "print(numeric_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe hyperparametric values for gridsearch. Change/update these values according to the volume/kind of numeric features\n",
    "params ={ \n",
    "   'min_samples_leaf':range(1000,10000,1000),\n",
    "   'max_depth':[2,3],\n",
    "   'min_impurity_decrease':[0,0.001, 0.005, 0.01, 0.05],\n",
    "   'min_samples_split' : range(500, 10000, 500),\n",
    "   'max_features' : [1, 2, 3],\n",
    "   'splitter' : ['best', 'random']\n",
    "}\n",
    "\n",
    "# feature binning\n",
    "\n",
    "numeric_data = feature_bins('numeric', numeric_data, params, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of tables containing binned features and their min, max values\n",
    "Tree_Bins = []\n",
    "for i in numeric_cols:\n",
    "    Tree_Bins.append(numeric_data[[i, i+'_Bins']].dropna().groupby(i+'_Bins').agg([np.min, np.max]).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table containing features and their respective number of bins created\n",
    "\n",
    "Bins=[]\n",
    "for i in range(len(Tree_Bins)):\n",
    "    Bins.append(tuple((Tree_Bins[i].columns[1][0],len(Tree_Bins[i].index.values))))\n",
    "    \n",
    "Bins_Table = pd.DataFrame(Bins, columns= ['Feature','Bins'])\n",
    "Bins_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"date\" and 'week' columns based on time_column for stability plots\n",
    "\n",
    "# numeric_data[target] = numeric_data[target].astype(float)\n",
    "numeric_data['date']= pd.to_datetime(numeric_data[time_column]).dt.date\n",
    "numeric_data['week']= pd.to_datetime(numeric_data[time_column]).dt.week\n",
    "\n",
    "# feature named to be used in creating stability plots\n",
    "\n",
    "names = []\n",
    "for j in range(len(Tree_Bins)):\n",
    "    names.append(Tree_Bins[j].index.name)\n",
    "\n",
    "# Plotting stability plots and savinf in variable \"figs\"\n",
    "fig_list = []\n",
    "figs = plotting(numeric_data, names, prefix, fig_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables describing min, max values of each bin for each feature\n",
    "\n",
    "for i, j in enumerate(Tree_Bins):\n",
    "    j=j.reset_index()\n",
    "    j = j.rename(columns = {j.index.name:'Bins'})\n",
    "    fig, ax =plt.subplots(1,1)\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=j.values,colLabels=j.columns,loc=\"center\", colWidths = [0.5, 0.5, 0.5], cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create case statements to be plugged-in to create binned features in SQL\n",
    "# case statements for original numerical cols and numerical datetime cols are created separately\n",
    "\n",
    "# remove numerical datetime cols\n",
    "if len(datetime_cols) >= 1:\n",
    "    for col in processed_datetime_cols:\n",
    "        if col in numeric_cols:\n",
    "            numeric_cols.remove(col)\n",
    "\n",
    "# update the numeric_cols names with \"_Bins\"\n",
    "leaf_num_cols = []\n",
    "for col in numeric_cols:\n",
    "    leaf_num_cols.append(col+str('_Bins'))\n",
    "        \n",
    "# case statements for each feature will be created according to the following logic\n",
    "# first line is for NULL values given NA\n",
    "# first bin will have \"<=\" for max bin value. For eg. if bin_1 has min = -1.0 and max = 5.0, then it will give statement \"... When feature_1 <= 5.0 then '<=5.0' ...\"\n",
    "# second onward bins will be given bin values either '=' if bin is created with just 1 value (might happen when just 1 value has alot of occurences), it will give statement \"... When feature_1 = 5.0 then '5.0' ...\"\n",
    "# or if its min and max values are different then it will create statement \"... When feature_1 bewtween 5.0 and 10.0 then '5.0 - 10.0' ...\"\n",
    "# last bin statement will have '>=' for min bin values. for eg. if bin_5 has min = 100.0 and max = 500.0 then it will give statement \"... When feature_1 >=100.0 then '>=100.0' ...\"\n",
    "# statement ends with dataset_name_bin_feature_name. Chnage if other naming convention is used. Also remember to update this change in create_cerise_json function as well\n",
    "\n",
    "with open(statement_filename+\".txt\",\"w\") as f:\n",
    "    for i in range(len(Tree_Bins)):\n",
    "        if Tree_Bins[i].index.name in leaf_num_cols:\n",
    "            bin_values = Tree_Bins[i]\n",
    "            j = len(bin_values)\n",
    "            print( \"(CASE WHEN (IFNULL(\",numeric_cols[i],\", 'NA') = 'NA') THEN 'NA'\",file=f)\n",
    "            if bin_values.columns[1][1] == 'amax':\n",
    "                print(\"WHEN \",numeric_cols[i],\" <= \",bin_values.iloc[0][1], \" THEN \"'\" <= ',bin_values.iloc[0][1],'\"',file=f)\n",
    "                if j >= 2:\n",
    "                    for m in range(j-2):\n",
    "                        if bin_values.iloc[m+1][0] == bin_values.iloc[m+1][1]:\n",
    "                            print(\"WHEN \",numeric_cols[i],\" = \" ,bin_values.iloc[m+1][1] ,\" THEN \"'\"',bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                        else:\n",
    "                            print(\"WHEN \",numeric_cols[i],\" BETWEEN \" ,bin_values.iloc[m+1][0] ,\" AND \", bin_values.iloc[m+1][1],\" THEN \"'\"',bin_values.iloc[m+1][0],\"-\",bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                    print(\"WHEN \",numeric_cols[i],\" >=\" ,bin_values.iloc[j-1][0],\" THEN \"'\" >= ',bin_values.iloc[j-1][0],'\"',file=f)\n",
    "                else:\n",
    "                    print(\"WHEN \",numeric_cols[i],\" >=\" ,bin_values.iloc[1][0],\" THEN \"'\" >= ',bin_values.iloc[1][0],'\"',file=f)\n",
    "            else:\n",
    "                print(\"WHEN \",numeric_cols[i],\" <= \",bin_values.iloc[0][1], \" THEN \"'\" <= ',bin_values.iloc[0][1],'\"',file=f)\n",
    "                if j>=2:\n",
    "                    for m in range(j-2):\n",
    "                        if bin_values.iloc[m+1][0] == bin_values.iloc[m+1][1]:\n",
    "                            print(\"WHEN \",numeric_cols[i],\" = \" ,bin_values.iloc[m+1][1] ,\" THEN \"'\" ',bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                        else:\n",
    "                            print(\"WHEN \",numeric_cols[i],\" BETWEEN \" ,bin_values.iloc[m+1][1] ,\" AND \", bin_values.iloc[m+1][0],\" THEN \"'\"',bin_values.iloc[m+1][1],\"-\",bin_values.iloc[m+1][0],'\"',file=f)\n",
    "                    print(\"WHEN \",numeric_cols[i],\" >=\" ,bin_values.iloc[j-1][1],\" THEN \"'\" >= ',bin_values.iloc[j-1][1],'+\"',file=f)\n",
    "                else:\n",
    "                    print(\"WHEN \",numeric_cols[i],\" >=\" ,bin_values.iloc[1][1],\" THEN \"'\" >= ',bin_values.iloc[1][1],'+\"',file=f)\n",
    "            print(\"END) AS \",dataset_name+str(\"_\")+\"bin_%s\"%numeric_cols[i],\",\",file=f)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case statements for dnumeric datetime features are created seperately here\n",
    "# Logic is same as numeric features with some additions (time duration difference) \n",
    "\n",
    "if len(datetime_cols) == 0:\n",
    "    print('No processed datetime columns available to be binned')\n",
    "else:\n",
    "    leaf_date_cols = []\n",
    "    for col in processed_datetime_cols:\n",
    "        leaf_date_cols.append(col+str('_Bins'))\n",
    "\n",
    "    leaf_date = pd.DataFrame({'processed_datetime_cols' : processed_datetime_cols,\n",
    "                             'tree_datetime_cols' : leaf_date_cols})\n",
    "\n",
    "    leaf_date['type'] = leaf_date['processed_datetime_cols'].str.split('_since').str[0]\n",
    "    leaf_date['type'] = [c.upper() for c in leaf_date['type']]\n",
    "\n",
    "    with open(statement_filename+\".txt\",\"a\") as f:\n",
    "        for i in range(len(Tree_Bins)):\n",
    "            for k in range(len(leaf_date)):\n",
    "                if Tree_Bins[i].index.name == leaf_date['tree_datetime_cols'][k]:\n",
    "                    bin_values = Tree_Bins[i]\n",
    "                    j = len(bin_values)\n",
    "                    print( \"(CASE WHEN (IFNULL(\",leaf_date['processed_datetime_cols'][k],\", 'NA') = 'NA') THEN 'NA'\",file=f)\n",
    "                    if bin_values.columns[1][1] == 'amax':\n",
    "                        print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") <= \",bin_values.iloc[0][1], \" THEN \"'\" <= ',bin_values.iloc[0][1],'\"',file=f)\n",
    "                        if j >= 2:\n",
    "                            for m in range(j-2):\n",
    "                                if bin_values.iloc[m+1][0] == bin_values.iloc[m+1][1]:\n",
    "                                    print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") = \" ,bin_values.iloc[m+1][1] ,\" THEN \"'\"',bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                                else:\n",
    "                                    print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") BETWEEN \" ,bin_values.iloc[m+1][0] ,\" AND \", bin_values.iloc[m+1][1],\" THEN \"'\"',bin_values.iloc[m+1][0],\"-\",bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                            print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") >=\" ,bin_values.iloc[j-1][0],\" THEN \"'\" >= ',bin_values.iloc[j-1][0],'\"',file=f)\n",
    "                        else:\n",
    "                            print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") >=\" ,bin_values.iloc[1][0],\" THEN \"'\" >= ',bin_values.iloc[1][0],'\"',file=f)\n",
    "                    else:\n",
    "                        print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") <= \",bin_values.iloc[0][1], \" THEN \"'\" <= ',bin_values.iloc[0][1],'\"',file=f)\n",
    "                        if j>=2:\n",
    "                            for m in range(j-2):\n",
    "                                if bin_values.iloc[m+1][0] == bin_values.iloc[m+1][1]:\n",
    "                                    print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") = \" ,bin_values.iloc[m+1][1] ,\" THEN \"'\" ',bin_values.iloc[m+1][1],'\"',file=f)\n",
    "                                else:\n",
    "                                    print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") BETWEEN \" ,bin_values.iloc[m+1][1] ,\" AND \", bin_values.iloc[m+1][0],\" THEN \"'\"',bin_values.iloc[m+1][1],\"-\",bin_values.iloc[m+1][0],'\"',file=f)\n",
    "                            print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") >=\" ,bin_values.iloc[j-1][1],\" THEN \"'\" >= ',bin_values.iloc[j-1][1],'+\"',file=f)\n",
    "                        else:\n",
    "                            print(\"WHEN 1.0* TIMESTAMPDIFF(\",leaf_date['type'][k],\",\",leaf_date['processed_datetime_cols'][k],\") >=\" ,bin_values.iloc[1][1],\" THEN \"'\" >= ',bin_values.iloc[1][1],'+\"',file=f)\n",
    "                    print(\"END) AS \",dataset_name+str(\"_\")+\"bin_%s\"%leaf_date['processed_datetime_cols'][k],\",\",file=f)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categroical features are combination of categroical and mixed features (treating both as categorical) and removing any duplicate feature\n",
    "cat_cols = categorical_cols + mixed_cols\n",
    "\n",
    "cate_cols=[]\n",
    "for item in cat_cols:\n",
    "    if item not in cate_cols:\n",
    "        cate_cols.append(item)\n",
    "\n",
    "# make new data table with just categroical cols, calltime and target metrics\n",
    "cate_data = data[cate_cols+[target]+[time_column]]\n",
    "cate_data[target] = cate_data[target].astype(float)\n",
    "cate_data['Num'] = cate_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing categorical features; change dtype to 'str', apply smalls threshold\n",
    "\n",
    "smalls_thresh = 0.005\n",
    "\n",
    "for f in cate_cols:\n",
    "    cate_data[f] = cate_data[f].astype(str)\n",
    "    cate_data[f] = cate_data[f].replace(nas_to_replace, np.nan)\n",
    "    counts = cate_data[f].value_counts(normalize=True)\n",
    "    mapping = cate_data[f].map(counts)\n",
    "    cate_data[f] = cate_data[f].mask(mapping < smalls_thresh, 'smalls')\n",
    "print('After processing')\n",
    "print(cate_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe hyperparametric values for gridsearch. Change/update these values according to the volume/kind of categoric features.\n",
    "\n",
    "params ={ \n",
    "   'min_samples_leaf':range(1000,10000,1000),\n",
    "   'max_depth':[2,3],\n",
    "   'min_impurity_decrease':[0,0.001, 0.005, 0.01, 0.05],\n",
    "   'min_samples_split' : range(500, 10000, 500),\n",
    "   'max_features' : [1, 2, 3],\n",
    "   'splitter' : ['best', 'random']\n",
    "}\n",
    "\n",
    "# feature binning\n",
    "# usually takes more time than numeric feature binning. Depends on hyperparametric values as well\n",
    "\n",
    "categorical_data = feature_bins('categorical', cate_data, params, cate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table containing categoric features with binned values using groupby function\n",
    "\n",
    "Tree_Bins = []\n",
    "for i in cate_cols:\n",
    "    Tree_Bins = categorical_data[['Num', i , i+'_Tree_Leaf']]\n",
    "#     Tree_Bins = Tree_Bins[~(Tree_Bins[i].isna()|Tree_Bins[i].isnull()|(Tree_Bins[i] == 'nan'))]\n",
    "    Tree_Bins.dropna(inplace=True)\n",
    "    Tree_Bins[i+'_bins'] = Tree_Bins.groupby(i+'_Tree_Leaf')[i].transform(lambda x: ','.join(\"'\"+item+\"'\" for item in set(x)))\n",
    "    categorical_data = pd.merge(categorical_data, Tree_Bins[['Num', i+'_bins']], on='Num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table containing categorical features, number of bins created and bin values comma seperated\n",
    "\n",
    "bins = []\n",
    "for i in cate_cols:\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for j in range(categorical_data[i+'_Tree_Leaf'].dropna().nunique()):\n",
    "        list1.append([categorical_data[i+'_bins'].dropna().unique()[j]])\n",
    "    bins.append(tuple((i, categorical_data[i+'_Tree_Leaf'].dropna().nunique(), list1)))\n",
    "    \n",
    "Bins_Table = pd.DataFrame(bins, columns= ['Categorical_Feature','Bins_value', 'Bins'])\n",
    "\n",
    "Bins_Table.set_index('Categorical_Feature', inplace=True)\n",
    "Bins_Table.reset_index(inplace=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "Bins_Table[['Categorical_Feature','Bins_value', 'Bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create case statement for categorical features that will update with numerical features case statement\n",
    "# if seperate file is needed, then change 'a' to 'w' in the first line below and change name of the file as needed\n",
    "# case statement for each feature will be created according to the following logic\n",
    "# first line is for NULL values given NA\n",
    "# bin names are automatically given as \"Bin_0\", \"Bin_1\",... according to the number of bins created\n",
    "# statement ends with dataset_name_bin_feature_name. Change if other naming convention is used. Also remember to update this change in create_cerise_json function as well\n",
    "\n",
    "with open(statement_filename+\".txt\",\"a\") as f:\n",
    "    for i in range(len(Bins_Table)):\n",
    "        bin_values = Bins_Table.iloc[i]\n",
    "        k = bin_values['Bins_value']\n",
    "#         bin_values['Bins_comma'].dropna().unique().to_list()\n",
    "        print( \"(CASE WHEN (IFNULL(\",bin_values['Categorical_Feature'],\", 'NA') = 'NA') THEN 'NA'\",file=f)\n",
    "        print(\"WHEN \",bin_values['Categorical_Feature'],\" in (\",bin_values['Bins'][0][0], \") THEN '\" , str('Bin_0'),\"'\",file=f)\n",
    "        if k >= 2:\n",
    "            for m in range(k-1):\n",
    "                print(\"WHEN \",bin_values['Categorical_Feature'],\" in (\",bin_values['Bins'][m+1][0], \") THEN '\" , str('Bin_')+str(m+1),\"'\",file=f)\n",
    "        print(\"else 'smalls' END) AS \",dataset_name+\"_bin_%s\"%bin_values['Categorical_Feature'], \",\",file=f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"date\" and \"week\" columns based on time_column for stability plots\n",
    "\n",
    "# categorical_data[target] = categorical_data[target].astype(float)\n",
    "categorical_data[time_column] = pd.to_datetime(categorical_data[time_column])\n",
    "categorical_data['date']= categorical_data.calltime.dt.date\n",
    "categorical_data['week']= categorical_data.calltime.dt.week\n",
    "\n",
    "# features names to be used in creating stability plots\n",
    "\n",
    "names = []\n",
    "\n",
    "for j in range(len(Bins_Table)):\n",
    "    names.append(Bins_Table['Categorical_Feature'][j]+'_Tree_Leaf')\n",
    "\n",
    "# Plotting stability plots and saving in variable \"figs\"\n",
    "    \n",
    "figs = plotting(categorical_data, names, prefix, figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability Plots for both Numerical and Categorical binned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating HTML file for combined stability plots of numerical and categorical features\n",
    "\n",
    "figures_to_html(figs, './Stability_binned_features.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
